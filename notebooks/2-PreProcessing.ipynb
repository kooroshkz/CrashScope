{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b38625d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 382,421 records with 40 columns\n"
     ]
    }
   ],
   "source": [
    "# CrashScope Data Preprocessing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"CrashScope\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Setup paths\n",
    "ROOT = Path.cwd()\n",
    "RAW_DIR = ROOT.parent / \"datasets\" / \"RawData\" \n",
    "PROCESSED_DIR = ROOT.parent / \"datasets\" / \"ProcessedData\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "df = spark.read.parquet(str(RAW_DIR / \"accidents_2022_2024_full.parquet\"))\n",
    "print(f\"Loaded {df.count():,} records with {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "531c1f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records after preprocessing: 238,525\n",
      "Features: 10 (5 numeric + 5 categorical)\n"
     ]
    }
   ],
   "source": [
    "# Feature selection and data cleaning\n",
    "features = {\n",
    "    'numeric': ['jaar_ongeval', 'aantal_partijen', 'maximum_snelheid', 'lon', 'lat'],\n",
    "    'categorical': ['verkeersongeval_afloop', 'aard_ongeval', 'bebouwde_kom', 'wegverlichting', 'weersgesteldheid']\n",
    "}\n",
    "\n",
    "# Clean missing values\n",
    "df_clean = df\n",
    "for col_name in [col_name for col_name, dtype in dict(df.dtypes).items() if dtype == 'string']:\n",
    "    df_clean = df_clean.withColumn(col_name, \n",
    "        when((col(col_name) == \"\") | (col(col_name) == \"Unknown\") | \n",
    "             (col(col_name) == \"NULL\") | (col(col_name) == \"N/A\"), None)\n",
    "        .otherwise(col(col_name)))\n",
    "\n",
    "# Select features and remove nulls\n",
    "all_features = features['numeric'] + features['categorical']\n",
    "df_processed = df_clean.select(*all_features).dropna()\n",
    "\n",
    "print(f\"Records after preprocessing: {df_processed.count():,}\")\n",
    "print(f\"Features: {len(all_features)} ({len(features['numeric'])} numeric + {len(features['categorical'])} categorical)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0569cb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML pipeline fitted. Feature vector size: 33\n"
     ]
    }
   ],
   "source": [
    "# Build ML pipeline\n",
    "stages = []\n",
    "\n",
    "# Encode categorical features\n",
    "for col_name in features['categorical']:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_idx\", handleInvalid=\"skip\")\n",
    "    encoder = OneHotEncoder(inputCol=f\"{col_name}_idx\", outputCol=f\"{col_name}_vec\")\n",
    "    stages.extend([indexer, encoder])\n",
    "\n",
    "# Create feature vector\n",
    "feature_cols = features['numeric'] + [f\"{col}_vec\" for col in features['categorical']]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\", handleInvalid=\"skip\")\n",
    "stages.append(assembler)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "stages.append(scaler)\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "model = pipeline.fit(df_processed)\n",
    "df_ml_ready = model.transform(df_processed)\n",
    "\n",
    "print(f\"ML pipeline fitted. Feature vector size: {df_ml_ready.select('features').first()['features'].size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a960142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved:\n",
      "- ml_ready.parquet: 238,525 records (USE THIS FOR ML)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- train.parquet: 190,829 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 864:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test.parquet: 47,696 records\n",
      "- preprocessing_model: fitted pipeline for new data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save preprocessed data\n",
    "train, test = df_ml_ready.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Save ML-ready dataset (this is what you use for ML)\n",
    "df_ml_ready.select(*all_features, \"features\").write.mode('overwrite').parquet(str(PROCESSED_DIR / \"ml_ready.parquet\"))\n",
    "\n",
    "# Save train/test splits\n",
    "train.select(*all_features, \"features\").write.mode('overwrite').parquet(str(PROCESSED_DIR / \"train.parquet\"))\n",
    "test.select(*all_features, \"features\").write.mode('overwrite').parquet(str(PROCESSED_DIR / \"test.parquet\"))\n",
    "\n",
    "# Save preprocessing pipeline\n",
    "model.write().overwrite().save(str(PROCESSED_DIR / \"preprocessing_model\"))\n",
    "\n",
    "print(\"Files saved:\")\n",
    "print(f\"- ml_ready.parquet: {df_ml_ready.count():,} records (USE THIS FOR ML)\")\n",
    "print(f\"- train.parquet: {train.count():,} records\")\n",
    "print(f\"- test.parquet: {test.count():,} records\")\n",
    "print(f\"- preprocessing_model: fitted pipeline for new data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
